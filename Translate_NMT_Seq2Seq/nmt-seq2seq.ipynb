{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-08T08:37:30.104892Z","iopub.execute_input":"2021-08-08T08:37:30.105238Z","iopub.status.idle":"2021-08-08T08:37:30.118154Z","shell.execute_reply.started":"2021-08-08T08:37:30.105207Z","shell.execute_reply":"2021-08-08T08:37:30.116737Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"/kaggle/input/turkish-to-english-translator/TR2EN.txt\n/kaggle/input/glove6b/glove.6B.200d.txt\n/kaggle/input/glove6b/glove.6B.50d.txt\n/kaggle/input/glove6b/glove.6B.300d.txt\n/kaggle/input/glove6b/glove.6B.100d.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"data seti 450k dan oluşan bir dataset","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:37:30.119731Z","iopub.execute_input":"2021-08-08T08:37:30.120428Z","iopub.status.idle":"2021-08-08T08:37:30.125191Z","shell.execute_reply.started":"2021-08-08T08:37:30.120390Z","shell.execute_reply":"2021-08-08T08:37:30.124177Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.keras.models import Model\nfrom tensorflow.python.keras.layers import Input, Dense, GRU, Embedding, CuDNNGRU\nfrom keras.optimizers import RMSprop\nfrom tensorflow.python.keras.callbacks import ModelCheckpoint\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:37:30.126975Z","iopub.execute_input":"2021-08-08T08:37:30.127810Z","iopub.status.idle":"2021-08-08T08:37:30.134513Z","shell.execute_reply.started":"2021-08-08T08:37:30.127766Z","shell.execute_reply":"2021-08-08T08:37:30.133669Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"#model cümle üretmeye başlangıç tokenini gördükten sonra başlıyor. başlangıç tokeni datasette olmayan bir kelime olmalıdır.\n#cümlenin sonlanması için bitiş tokenini belirtmemiz gerekiyor.\n#boşlum karekterleri çok önemli !!!!\n\nmark_start = 'ssss '\nmark_end = ' eeee'","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:37:30.136021Z","iopub.execute_input":"2021-08-08T08:37:30.136689Z","iopub.status.idle":"2021-08-08T08:37:30.143345Z","shell.execute_reply.started":"2021-08-08T08:37:30.136632Z","shell.execute_reply":"2021-08-08T08:37:30.142638Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"data_src = []\ndata_dest = []","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:37:30.145012Z","iopub.execute_input":"2021-08-08T08:37:30.145785Z","iopub.status.idle":"2021-08-08T08:37:30.186498Z","shell.execute_reply.started":"2021-08-08T08:37:30.145735Z","shell.execute_reply":"2021-08-08T08:37:30.185693Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"for line in open('../input/turkish-to-english-translator/TR2EN.txt', encoding='UTF-8'):\n    en_text, tr_text = line.rstrip().split('\\t')\n    tr_text = mark_start + tr_text + mark_end\n    data_src.append(en_text)\n    data_dest.append(tr_text)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:37:30.188843Z","iopub.execute_input":"2021-08-08T08:37:30.189425Z","iopub.status.idle":"2021-08-08T08:37:30.978997Z","shell.execute_reply.started":"2021-08-08T08:37:30.189383Z","shell.execute_reply":"2021-08-08T08:37:30.978005Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"print(data_src[100])\nprint(data_dest[100])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:37:30.980612Z","iopub.execute_input":"2021-08-08T08:37:30.981232Z","iopub.status.idle":"2021-08-08T08:37:30.987635Z","shell.execute_reply.started":"2021-08-08T08:37:30.981187Z","shell.execute_reply":"2021-08-08T08:37:30.986594Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"I drove.\nssss Araba sürdüm. eeee\n","output_type":"stream"}]},{"cell_type":"code","source":"print(data_src[200000])\nprint(data_dest[200000])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:37:30.989123Z","iopub.execute_input":"2021-08-08T08:37:30.989466Z","iopub.status.idle":"2021-08-08T08:37:30.998830Z","shell.execute_reply.started":"2021-08-08T08:37:30.989432Z","shell.execute_reply":"2021-08-08T08:37:30.997941Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"Can you see anything missing?\nssss Eksik bir şey görebiliyor musun? eeee\n","output_type":"stream"}]},{"cell_type":"code","source":"class TokenizerWrap(Tokenizer):\n    def __init__(self, texts, padding, reverse=False, num_words=None):\n        Tokenizer.__init__(self, num_words=num_words) #tokenizer tanımladık\n        \n        self.fit_on_texts(texts) # aldığımız kelimeleri tokenleştirdik\n        \n        # tokenleri text haline getirmek için key ve valueların yerini değiştiriyoruz.\n        self.index_to_word = dict(zip(self.word_index.values(), self.word_index.keys()))\n        \n        self.tokens = self.texts_to_sequences(texts) # elimizdeki yazıları tokenler olarak bir listede topluyoruz.\n        \n        \n        #pre padding başa ekleme, post padding sona ekleme, pre truncating baştan çıkarma, post truncating sondan çıkarma\n        \n        if reverse:\n            self.tokens = [list(reversed(x)) for x in self.tokens]\n            truncating = 'pre'\n        else:\n            truncating = 'post'\n        \n            \n        self.num_tokens = [len(x) for x in self.tokens]\n        self.max_tokens = np.mean(self.num_tokens) + 2 * np.std(self.num_tokens)\n        self.max_tokens = int(self.max_tokens)\n        \n        \n        \n        \n        self.tokens_padded = pad_sequences(self.tokens,\n                                           maxlen=self.max_tokens,\n                                           padding=padding,\n                                           truncating=truncating)\n        \n        \n    def token_to_word(self, token): # bir sayıya denk gelen kelimeyi döndürüyoruz. \n        word = ' ' if token == 0 else self.index_to_word[token]  #token 0 ise boşluk döndür, değilse dicte key olarak ver valueyu al\n        return word\n    \n    def tokens_to_string(self, tokens): # token listesini cümleye çeviriyoruz. eğer token sıfır değilse çünkü paddingte sıfır eklemiştik\n        words = [self.index_to_word[token] for token in tokens if token != 0]\n        text = ' '.join(words)\n        return text\n    \n    \n    #text_to_tokens funk ile modele tek bir cümle vererek istediğimiz cümleyi modele uygun olarak vereceğiz.\n    # modele vereceğimiz cümleyi modele hazır hale getiemk için bir cümle yazalım.\n    \n    def text_to_tokens(self, text, padding, reverse=False): # modele cümleyi token olarak vermek için hazırlıyoruz.\n        tokens = self.texts_to_sequences([text]) # önce aldığımız texti tokenleştiriyoruz.\n        tokens = np.array(tokens) # daha sonra bu tokenleri numpy arraye çeviriyoruz.\n        \n        if reverse:\n            tokens = np.flip(tokens, axis=1) #satır üzerinde ters çevirme yapıyoruz.\n            truncating = 'pre'\n        else:\n            truncating = 'post'\n            \n        tokens = pad_sequences(tokens,\n                               maxlen=self.max_tokens,\n                               padding=padding,\n                               truncating=truncating)\n        \n        return tokens","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:37:31.001257Z","iopub.execute_input":"2021-08-08T08:37:31.001927Z","iopub.status.idle":"2021-08-08T08:37:31.035715Z","shell.execute_reply.started":"2021-08-08T08:37:31.001877Z","shell.execute_reply":"2021-08-08T08:37:31.034845Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"tokenizer_src = TokenizerWrap(texts=data_src,\n                              padding='pre',\n                              reverse=True,\n                              num_words=None)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:37:31.037403Z","iopub.execute_input":"2021-08-08T08:37:31.037750Z","iopub.status.idle":"2021-08-08T08:37:46.709787Z","shell.execute_reply.started":"2021-08-08T08:37:31.037714Z","shell.execute_reply":"2021-08-08T08:37:46.708919Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"tokenizer_dest = TokenizerWrap(texts=data_dest,\n                              padding='post',\n                              reverse=False,\n                              num_words=None)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:37:46.711031Z","iopub.execute_input":"2021-08-08T08:37:46.711362Z","iopub.status.idle":"2021-08-08T08:38:03.059115Z","shell.execute_reply.started":"2021-08-08T08:37:46.711329Z","shell.execute_reply":"2021-08-08T08:38:03.057970Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"# ÖNEMLİ AÇIKLAMA ","metadata":{}},{"cell_type":"markdown","source":"> Encodera vereceğimiz inputu tokenleştirme yaparak neden ters çeviriyoruz ?\n> \n> bunun iki türlü faydası var:\n>     birincisi truncating yaparken cümleyi başından kesiyoruz. eğer cümleyi ters çevirmezsek cümleyi kesince baştaki kelimeler gidecekti. \n>     \n>     bunun şöyle bir yararı var:\n>         encodera verdiğimiz inputta baştaki kelimelerin kesildiğini düşünelim. encoder çalışıp düşünce vektörü oluştuurp decodera verecek\n>         decoderda ise truncatingi post yaptık yani cümlenin sonundaki kelimeler kesiliyor. öyleyse encodera verilecek input bir cümlenin \n>         sonundaki kelimelerden oluşacak, decoderda ise bir cümlenin başındaki kelimelerden oluşacak. yani network ingilizce cümlelerin \n>         sonu   ile türkçe cümlelerin başını eşleştirecek bu şekilde doğru kelimeler birbirleri ile eşleşecek.\n>         \n>     ikinci avantajı:\n>         cümleleri encodera verince en sondan başlayarak başa doğru ilerleyecek encoder düşünce vektörü üretmeden önce gördüğü son şey\n>         cümlenin başı olacak. decodera geçtiğinde ise direk cümlenin başından başlayıp üretmeye başlayacak bu şekilde en son cümlenin başını \n>         görüp üretmeye başlarsa daha isabetli bir çeviri yapabilir.\n>     ","metadata":{}},{"cell_type":"code","source":"tokens_src = tokenizer_src.tokens_padded\ntokens_dest = tokenizer_dest.tokens_padded\nprint(tokens_src.shape) #ingilizcede cümle boyutumuz 11\nprint(tokens_dest.shape) #türkçede cümle boyutumuz 10","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.060800Z","iopub.execute_input":"2021-08-08T08:38:03.061256Z","iopub.status.idle":"2021-08-08T08:38:03.069229Z","shell.execute_reply.started":"2021-08-08T08:38:03.061217Z","shell.execute_reply":"2021-08-08T08:38:03.068374Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"(473035, 11)\n(473035, 10)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tokens_dest[200000])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.070444Z","iopub.execute_input":"2021-08-08T08:38:03.070974Z","iopub.status.idle":"2021-08-08T08:38:03.078063Z","shell.execute_reply.started":"2021-08-08T08:38:03.070937Z","shell.execute_reply":"2021-08-08T08:38:03.076956Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"[   1 2391    4   18 4127   48    2    0    0    0]\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer_dest.tokens_to_string(tokens_dest[200000])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.079628Z","iopub.execute_input":"2021-08-08T08:38:03.080058Z","iopub.status.idle":"2021-08-08T08:38:03.087310Z","shell.execute_reply.started":"2021-08-08T08:38:03.080023Z","shell.execute_reply":"2021-08-08T08:38:03.086430Z"},"trusted":true},"execution_count":92,"outputs":[{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"'ssss eksik bir şey görebiliyor musun eeee'"},"metadata":{}}]},{"cell_type":"code","source":"print(tokens_src[200000])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.088760Z","iopub.execute_input":"2021-08-08T08:38:03.089151Z","iopub.status.idle":"2021-08-08T08:38:03.096179Z","shell.execute_reply.started":"2021-08-08T08:38:03.089116Z","shell.execute_reply":"2021-08-08T08:38:03.095133Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"[   0    0    0    0    0    0 1028  113   95    5   39]\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer_src.tokens_to_string(tokens_src[200000])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.097440Z","iopub.execute_input":"2021-08-08T08:38:03.097877Z","iopub.status.idle":"2021-08-08T08:38:03.106453Z","shell.execute_reply.started":"2021-08-08T08:38:03.097765Z","shell.execute_reply":"2021-08-08T08:38:03.105580Z"},"trusted":true},"execution_count":94,"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"'missing anything see you can'"},"metadata":{}}]},{"cell_type":"code","source":"data_src[200000]","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.109039Z","iopub.execute_input":"2021-08-08T08:38:03.109377Z","iopub.status.idle":"2021-08-08T08:38:03.116393Z","shell.execute_reply.started":"2021-08-08T08:38:03.109352Z","shell.execute_reply":"2021-08-08T08:38:03.115483Z"},"trusted":true},"execution_count":95,"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"'Can you see anything missing?'"},"metadata":{}}]},{"cell_type":"code","source":"token_start = tokenizer_dest.word_index[mark_start.strip()] #word_indexte kelimelerin tokenleri bulunuyor.\ntoken_start","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.117702Z","iopub.execute_input":"2021-08-08T08:38:03.118080Z","iopub.status.idle":"2021-08-08T08:38:03.126378Z","shell.execute_reply.started":"2021-08-08T08:38:03.118048Z","shell.execute_reply":"2021-08-08T08:38:03.125297Z"},"trusted":true},"execution_count":96,"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"token_end = tokenizer_dest.word_index[mark_end.strip()]\ntoken_end","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.134555Z","iopub.execute_input":"2021-08-08T08:38:03.134825Z","iopub.status.idle":"2021-08-08T08:38:03.142165Z","shell.execute_reply.started":"2021-08-08T08:38:03.134800Z","shell.execute_reply":"2021-08-08T08:38:03.141117Z"},"trusted":true},"execution_count":97,"outputs":[{"execution_count":97,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"encoder_input_data = tokens_src  # encoder inputu","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.144677Z","iopub.execute_input":"2021-08-08T08:38:03.145087Z","iopub.status.idle":"2021-08-08T08:38:03.148802Z","shell.execute_reply.started":"2021-08-08T08:38:03.145054Z","shell.execute_reply":"2021-08-08T08:38:03.147575Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"decoder_input_data = tokens_dest[:, :-1] # baştan başla sondan bir eksiğe kadar git\ndecoder_output_data = tokens_dest[:, 1:] # 1 den başla sona kadar git.","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.150149Z","iopub.execute_input":"2021-08-08T08:38:03.150490Z","iopub.status.idle":"2021-08-08T08:38:03.156867Z","shell.execute_reply.started":"2021-08-08T08:38:03.150455Z","shell.execute_reply":"2021-08-08T08:38:03.155677Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"encoder_input_data[200000]","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.158377Z","iopub.execute_input":"2021-08-08T08:38:03.158773Z","iopub.status.idle":"2021-08-08T08:38:03.169463Z","shell.execute_reply.started":"2021-08-08T08:38:03.158706Z","shell.execute_reply":"2021-08-08T08:38:03.168734Z"},"trusted":true},"execution_count":100,"outputs":[{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"array([   0,    0,    0,    0,    0,    0, 1028,  113,   95,    5,   39],\n      dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"decoder_input_data[200000]","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.170812Z","iopub.execute_input":"2021-08-08T08:38:03.171157Z","iopub.status.idle":"2021-08-08T08:38:03.179051Z","shell.execute_reply.started":"2021-08-08T08:38:03.171123Z","shell.execute_reply":"2021-08-08T08:38:03.178086Z"},"trusted":true},"execution_count":101,"outputs":[{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"array([   1, 2391,    4,   18, 4127,   48,    2,    0,    0], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"decoder_output_data[200000]","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.180409Z","iopub.execute_input":"2021-08-08T08:38:03.180817Z","iopub.status.idle":"2021-08-08T08:38:03.188640Z","shell.execute_reply.started":"2021-08-08T08:38:03.180782Z","shell.execute_reply":"2021-08-08T08:38:03.187646Z"},"trusted":true},"execution_count":102,"outputs":[{"execution_count":102,"output_type":"execute_result","data":{"text/plain":"array([2391,    4,   18, 4127,   48,    2,    0,    0,    0], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"aşağıda görüldüğü üzere input ve outputta tek fark outputta başlangıç tokeni yok\n\ndecoderda inputtan alınan başlangıç tokeniyle eksik kelimesinin üretilmesi beklenir, daha sonra eksik kelimesi input olduğunda \nbir kelimesinin outputta olması gerekir bu olay bitiş tokenine kadar böyle devam eder","metadata":{}},{"cell_type":"code","source":"tokenizer_dest.tokens_to_string(decoder_input_data[200000])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.190108Z","iopub.execute_input":"2021-08-08T08:38:03.190558Z","iopub.status.idle":"2021-08-08T08:38:03.197886Z","shell.execute_reply.started":"2021-08-08T08:38:03.190521Z","shell.execute_reply":"2021-08-08T08:38:03.196827Z"},"trusted":true},"execution_count":103,"outputs":[{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"'ssss eksik bir şey görebiliyor musun eeee'"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer_dest.tokens_to_string(decoder_output_data[200000])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.199314Z","iopub.execute_input":"2021-08-08T08:38:03.199722Z","iopub.status.idle":"2021-08-08T08:38:03.206542Z","shell.execute_reply.started":"2021-08-08T08:38:03.199685Z","shell.execute_reply":"2021-08-08T08:38:03.205455Z"},"trusted":true},"execution_count":104,"outputs":[{"execution_count":104,"output_type":"execute_result","data":{"text/plain":"'eksik bir şey görebiliyor musun eeee'"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer_src.tokens_to_string(encoder_input_data[200000])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.209056Z","iopub.execute_input":"2021-08-08T08:38:03.209335Z","iopub.status.idle":"2021-08-08T08:38:03.218177Z","shell.execute_reply.started":"2021-08-08T08:38:03.209304Z","shell.execute_reply":"2021-08-08T08:38:03.217329Z"},"trusted":true},"execution_count":105,"outputs":[{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"'missing anything see you can'"},"metadata":{}}]},{"cell_type":"code","source":"num_encoder_words = len(tokenizer_src.word_index)  # eng kelime sayı\nnum_decoder_words = len(tokenizer_dest.word_index) #tr kelime sayı","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.220506Z","iopub.execute_input":"2021-08-08T08:38:03.221189Z","iopub.status.idle":"2021-08-08T08:38:03.225295Z","shell.execute_reply.started":"2021-08-08T08:38:03.221149Z","shell.execute_reply":"2021-08-08T08:38:03.224469Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"print(num_encoder_words)\nprint(num_decoder_words)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.226505Z","iopub.execute_input":"2021-08-08T08:38:03.227054Z","iopub.status.idle":"2021-08-08T08:38:03.234952Z","shell.execute_reply.started":"2021-08-08T08:38:03.227012Z","shell.execute_reply":"2021-08-08T08:38:03.234065Z"},"trusted":true},"execution_count":107,"outputs":[{"name":"stdout","text":"21315\n94058\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ENCODER","metadata":{}},{"cell_type":"markdown","source":"**Embedding layerda kelime vektörlerini oluşturuken GLOVE kullanacağız.**","metadata":{}},{"cell_type":"markdown","source":"elimizde bulunan 21k lık kelimelere karşılık vektör bulamayabiliriz. tüm kelime vektörlerini rastgele tanımlıyoruz.\neğer glove kelime vektörleri içerisinde kelime haznemizdeki kelimelerin vektörleri bulunuyorsa o vektör rastgele oluşturulan vektörün yerine geçicek.\n\n400.000 kelimenin bulunduğu glove vektörünün içinde bizim kelimelerimiz bulunacaktır.","metadata":{}},{"cell_type":"code","source":"embedding_size = 100   # glove vektörlerinin uzunluğuda 100 ","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.236429Z","iopub.execute_input":"2021-08-08T08:38:03.237111Z","iopub.status.idle":"2021-08-08T08:38:03.241319Z","shell.execute_reply.started":"2021-08-08T08:38:03.237072Z","shell.execute_reply":"2021-08-08T08:38:03.240180Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"markdown","source":"**önce boş bir sözlük oluşturuyoruz, -->> word2vec={}**\n\n**daha sonra kelime ve vektörü olacak şekilde içini dolduruyoruz.**","metadata":{}},{"cell_type":"code","source":"word2vec = {}\nwith open('../input/glove6b/glove.6B.100d.txt', encoding='UTF-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:03.242924Z","iopub.execute_input":"2021-08-08T08:38:03.243650Z","iopub.status.idle":"2021-08-08T08:38:20.090896Z","shell.execute_reply.started":"2021-08-08T08:38:03.243611Z","shell.execute_reply":"2021-08-08T08:38:20.090045Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"#burada yaptığım işlem embedding vektörün içini dolduruyorum eğer kelime glove vektörünün içinde varsa o vektörü alıyorum\n\nembedding_matrix = np.random.uniform(-1, 1, (num_encoder_words, embedding_size))\nfor word, i in tokenizer_src.word_index.items():\n    if i < num_encoder_words:\n        embedding_vector = word2vec.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:20.092165Z","iopub.execute_input":"2021-08-08T08:38:20.092494Z","iopub.status.idle":"2021-08-08T08:38:20.157897Z","shell.execute_reply.started":"2021-08-08T08:38:20.092461Z","shell.execute_reply":"2021-08-08T08:38:20.156952Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"embedding_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:20.159288Z","iopub.execute_input":"2021-08-08T08:38:20.159690Z","iopub.status.idle":"2021-08-08T08:38:20.166179Z","shell.execute_reply.started":"2021-08-08T08:38:20.159638Z","shell.execute_reply":"2021-08-08T08:38:20.165084Z"},"trusted":true},"execution_count":111,"outputs":[{"execution_count":111,"output_type":"execute_result","data":{"text/plain":"(21315, 100)"},"metadata":{}}]},{"cell_type":"code","source":"encoder_input = Input(shape=(None,), name='encoder_input') # ınput layera int tokenleri veriyoruz. #shape none vererek boyutla ilgili sorunları çözüyoruz","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:20.167798Z","iopub.execute_input":"2021-08-08T08:38:20.168184Z","iopub.status.idle":"2021-08-08T08:38:20.176754Z","shell.execute_reply.started":"2021-08-08T08:38:20.168144Z","shell.execute_reply":"2021-08-08T08:38:20.175532Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"#kerasın embedding layerına ekliyoruz.\nencoder_embedding = Embedding(input_dim=num_encoder_words, \n                              output_dim=embedding_size,\n                              weights=[embedding_matrix],\n                              trainable=True,\n                              name='encoder_embedding')","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:20.178323Z","iopub.execute_input":"2021-08-08T08:38:20.178763Z","iopub.status.idle":"2021-08-08T08:38:20.187273Z","shell.execute_reply.started":"2021-08-08T08:38:20.178699Z","shell.execute_reply":"2021-08-08T08:38:20.186029Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"state_size = 256 # butün GRU layerlar 256 boyutlu output üretecek. daha yüksek bir state_size ile daha iyi sonuç alınır.","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:20.188700Z","iopub.execute_input":"2021-08-08T08:38:20.189204Z","iopub.status.idle":"2021-08-08T08:38:20.193686Z","shell.execute_reply.started":"2021-08-08T08:38:20.189170Z","shell.execute_reply":"2021-08-08T08:38:20.192808Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"#3 katmanlı bir GRU yapısı oluşturuyoruz.\nencoder_gru1 = GRU(state_size, name='encoder_gru1', return_sequences=True)\nencoder_gru2 = GRU(state_size, name='encoder_gru2', return_sequences=True)\nencoder_gru3 = GRU(state_size, name='encoder_gru3', return_sequences=False)  #düşünce vektörünü oluşturuyor.","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:20.194997Z","iopub.execute_input":"2021-08-08T08:38:20.195540Z","iopub.status.idle":"2021-08-08T08:38:20.218650Z","shell.execute_reply.started":"2021-08-08T08:38:20.195504Z","shell.execute_reply":"2021-08-08T08:38:20.217956Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"#layerları connect_encoder()' da zincir gibi bağlıyoruz.\ndef connect_encoder():\n    net = encoder_input\n    \n    net = encoder_embedding(net)\n    \n    net = encoder_gru1(net)\n    net = encoder_gru2(net)\n    net = encoder_gru3(net)\n    \n    encoder_output = net\n    \n    return encoder_output\n\n\n# encoder_output ile düşünce vektörünü oluşturmuş olduk . artık bu fonk çağırarak düşünce vektörüne ulaşabiliriz.","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:20.219704Z","iopub.execute_input":"2021-08-08T08:38:20.220032Z","iopub.status.idle":"2021-08-08T08:38:20.229692Z","shell.execute_reply.started":"2021-08-08T08:38:20.220000Z","shell.execute_reply":"2021-08-08T08:38:20.228899Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"encoder_output = connect_encoder()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:20.230784Z","iopub.execute_input":"2021-08-08T08:38:20.231126Z","iopub.status.idle":"2021-08-08T08:38:20.704670Z","shell.execute_reply.started":"2021-08-08T08:38:20.231090Z","shell.execute_reply":"2021-08-08T08:38:20.703747Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":"# DECODER","metadata":{}},{"cell_type":"markdown","source":"Encodera ingilizce cümleleri Decodera bu ingilizce cümlelerin türkçe çevirilerini veriyoruz.\n\neğitimde decodera düşünce vektörünü ve ingilizce cümleye karşılık gelen türkçe cümleyi veriyoruz ve modeli eğitiyoruz.\n\neğitim tamamlandıktan sonra encodera verdiğimiz ing cümlenin sonuda aldığımız düşünce vektörünü başlanğıç tokeni ile birlikte decodera vereceğiz.","metadata":{}},{"cell_type":"code","source":"decoder_initial_state = Input(shape=(state_size,), name='decoder_initial_state')\n    \n# decodera vereceğimiz düşünce vektörü için bir input layer oluştuuryoruz.","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:20.706034Z","iopub.execute_input":"2021-08-08T08:38:20.706411Z","iopub.status.idle":"2021-08-08T08:38:20.714070Z","shell.execute_reply.started":"2021-08-08T08:38:20.706373Z","shell.execute_reply":"2021-08-08T08:38:20.712966Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"decoder_input = Input(shape=(None,), name='decoder_input') \n\n# bu input ise decodera vereceğimiz türkçe cümle için, shape ' e none veriyoruz gelen her input kabulümüz.","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:20.715854Z","iopub.execute_input":"2021-08-08T08:38:20.716307Z","iopub.status.idle":"2021-08-08T08:38:20.724146Z","shell.execute_reply.started":"2021-08-08T08:38:20.716273Z","shell.execute_reply":"2021-08-08T08:38:20.723269Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"#normalde glove ile embedding matrisi oluşturuyoduk ama elimizdeki glove datası ing bizim decoderımızda türkçe veriler var\n#ya yeni bir glove modeli eğiteceğiz yada yaptığım gibi rastgele dolduracağız.\n\ndecoder_embedding = Embedding(input_dim=num_decoder_words,\n                              output_dim=embedding_size,\n                              name='decoder_embedding')","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:20.725593Z","iopub.execute_input":"2021-08-08T08:38:20.725999Z","iopub.status.idle":"2021-08-08T08:38:20.734125Z","shell.execute_reply.started":"2021-08-08T08:38:20.725963Z","shell.execute_reply":"2021-08-08T08:38:20.733351Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"decoder_gru1 = GRU(state_size, name='decoder_gru1', return_sequences=True)\ndecoder_gru2 = GRU(state_size, name='decoder_gru2', return_sequences=True)\ndecoder_gru3 = GRU(state_size, name='decoder_gru3', return_sequences=True)\n\n# decoderda üçüncü layera true dedik çünkü buradan bize bir cümle dönüyor, encoderda bize bir vektör dönüyordu.","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:20.736486Z","iopub.execute_input":"2021-08-08T08:38:20.736873Z","iopub.status.idle":"2021-08-08T08:38:20.753230Z","shell.execute_reply.started":"2021-08-08T08:38:20.736843Z","shell.execute_reply":"2021-08-08T08:38:20.752462Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"# decoderın çalışması sonucunda kelimeler elde etmek istiyoruz. suan layerın çalışması durumunda 256 boyutunda vektörler elde edeceğiz.\n# bu vektörleri kelimeye çevirmemiz lazım bunun için dense layer oluşturuyoruz. GRUdan dönen sonucu one hot türünde arraye dönüştüreceğiz.\n# bu vektörde en büyük değere sahip elemanın indexi output olarak hangi kelimenin verilmesi gerektiğini gösteriyor.\n#aktivasyonu linear yapınca değerler olduğu gibi geçiyor.\n\n\ndecoder_dense = Dense(num_decoder_words,\n                      activation='linear',\n                      name='decoder_output')","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:20.754540Z","iopub.execute_input":"2021-08-08T08:38:20.754954Z","iopub.status.idle":"2021-08-08T08:38:20.763540Z","shell.execute_reply.started":"2021-08-08T08:38:20.754918Z","shell.execute_reply":"2021-08-08T08:38:20.762833Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"#initial_state encoderın ürettiği düşünce vektörünü veriyoruz.\n\ndef connect_decoder(initial_state):\n    net = decoder_input\n    \n    net = decoder_embedding(net)\n    \n    net = decoder_gru1(net, initial_state=initial_state)\n    net = decoder_gru2(net, initial_state=initial_state)\n    net = decoder_gru3(net, initial_state=initial_state)\n    \n    decoder_output = decoder_dense(net)\n    \n    return decoder_output","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:20.765186Z","iopub.execute_input":"2021-08-08T08:38:20.765427Z","iopub.status.idle":"2021-08-08T08:38:20.775566Z","shell.execute_reply.started":"2021-08-08T08:38:20.765404Z","shell.execute_reply":"2021-08-08T08:38:20.774578Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"decoder_output = connect_decoder(initial_state=encoder_output)  # encoder_outputu (düşünce vektörünü ) -> decodera veriyoruz.\n\nmodel_train = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output]) \n\n#decodera iki farklı input veriyoruz bunlardan ilki düşünce vektörü diğeri türkçe cümle.\n#model_train ile encoder ve decoder birbirleri ile bağlanmış oldu.","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:20.777082Z","iopub.execute_input":"2021-08-08T08:38:20.777539Z","iopub.status.idle":"2021-08-08T08:38:21.187801Z","shell.execute_reply.started":"2021-08-08T08:38:20.777501Z","shell.execute_reply":"2021-08-08T08:38:21.186983Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"model_encoder = Model(inputs=[encoder_input], outputs=[encoder_output])    # çeviri için yapıyoruz.","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:21.189085Z","iopub.execute_input":"2021-08-08T08:38:21.189436Z","iopub.status.idle":"2021-08-08T08:38:21.196583Z","shell.execute_reply.started":"2021-08-08T08:38:21.189398Z","shell.execute_reply":"2021-08-08T08:38:21.195647Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"# iki farklı model daha tanımlayacağız bunları ise model tamamlandıktan sonra çeviri için kullancağız. \n\ndecoder_output = connect_decoder(initial_state=decoder_initial_state)\nmodel_decoder = Model(inputs=[decoder_input, decoder_initial_state], outputs=[decoder_output])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:21.198032Z","iopub.execute_input":"2021-08-08T08:38:21.198722Z","iopub.status.idle":"2021-08-08T08:38:21.528095Z","shell.execute_reply.started":"2021-08-08T08:38:21.198667Z","shell.execute_reply":"2021-08-08T08:38:21.527083Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"#üretilen türkçe kelimeyi gerçekteki türkçe kelime ile karşılaştırmalıyız \n#şimdi her kelime için one hot vektör tanımlayıp bunun karşılaştırılması yapılabilir ama inanılmaz büyük bir boyuta sahip olucak\n# bu yüzden de bunu kullanmak yerine spars cross entropy kullanıyoruz.\n\n# sparse cross entropy --> her iterasyonda kendi içerisinde intleri one hot vektörleri çeviriyor bu sayede kocaman bir matris oluşturmuyoruz.\n# spars entropyde softmaxten geçiyor bu yüzden decoderda dense layerda softmax kullanmadık. lineer kullandık\n\n\ndef sparse_cross_entropy(y_true, y_pred):\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n    loss_mean = tf.reduce_mean(loss)\n    return loss_mean\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:21.533321Z","iopub.execute_input":"2021-08-08T08:38:21.533616Z","iopub.status.idle":"2021-08-08T08:38:21.539074Z","shell.execute_reply.started":"2021-08-08T08:38:21.533586Z","shell.execute_reply":"2021-08-08T08:38:21.537724Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"optimizer = RMSprop(lr=1e-3)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:21.541474Z","iopub.execute_input":"2021-08-08T08:38:21.541945Z","iopub.status.idle":"2021-08-08T08:38:21.551807Z","shell.execute_reply.started":"2021-08-08T08:38:21.541906Z","shell.execute_reply":"2021-08-08T08:38:21.550949Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\n\n#import tensorflow as tf\ntf.compat.v1.disable_eager_execution()\n\n#tf.disable_v2_behavior()\n#x = tf.placeholder(shape=[None, None], dtype=tf.int32)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:21.553293Z","iopub.execute_input":"2021-08-08T08:38:21.553753Z","iopub.status.idle":"2021-08-08T08:38:21.561863Z","shell.execute_reply.started":"2021-08-08T08:38:21.553713Z","shell.execute_reply":"2021-08-08T08:38:21.560844Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"#decoder_target = tf.placeholder(dtype=tf.int32, shape=(None,None))\ndecoder_target=tf.compat.v1.placeholder(shape=[None, None], dtype=tf.int32)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:21.563377Z","iopub.execute_input":"2021-08-08T08:38:21.563809Z","iopub.status.idle":"2021-08-08T08:38:21.571747Z","shell.execute_reply.started":"2021-08-08T08:38:21.563759Z","shell.execute_reply":"2021-08-08T08:38:21.570987Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"#model_trainde encoder ve decoderı bağlamıştık zaten\n\nmodel_train.compile(optimizer=optimizer,\n                    loss=sparse_cross_entropy,\n                    target_tensors=[decoder_target])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:21.574371Z","iopub.execute_input":"2021-08-08T08:38:21.574690Z","iopub.status.idle":"2021-08-08T08:38:21.674476Z","shell.execute_reply.started":"2021-08-08T08:38:21.574643Z","shell.execute_reply":"2021-08-08T08:38:21.673627Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"#modelin eğitimi uzun süreceğinden kayıt ediyoruz.\n\npath_checkpoint = 'checkpoint.keras'\ncheckpoint = ModelCheckpoint(filepath=path_checkpoint, save_weights_only=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:21.676681Z","iopub.execute_input":"2021-08-08T08:38:21.677301Z","iopub.status.idle":"2021-08-08T08:38:21.681886Z","shell.execute_reply.started":"2021-08-08T08:38:21.677250Z","shell.execute_reply":"2021-08-08T08:38:21.681005Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"try:\n    model_train.load_weights(path_checkpoint)\nexcept Exception as error:\n    print('Checkpoint yüklenirken hata oluştu. Eğitime sıfırdan başlanıyor.')\n    print(error)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:21.683218Z","iopub.execute_input":"2021-08-08T08:38:21.683694Z","iopub.status.idle":"2021-08-08T08:38:21.694972Z","shell.execute_reply.started":"2021-08-08T08:38:21.683639Z","shell.execute_reply":"2021-08-08T08:38:21.693987Z"},"trusted":true},"execution_count":133,"outputs":[{"name":"stdout","text":"Checkpoint yüklenirken hata oluştu. Eğitime sıfırdan başlanıyor.\nUnable to open file (unable to open file: name = 'checkpoint.keras', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# modele vereceğimiz inputları belirtiyoruz. bu inputlar int tokenlerden oluşuyor.\n\nx_data = {'encoder_input': encoder_input_data, 'decoder_input': decoder_input_data}","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:21.696563Z","iopub.execute_input":"2021-08-08T08:38:21.697047Z","iopub.status.idle":"2021-08-08T08:38:21.703442Z","shell.execute_reply.started":"2021-08-08T08:38:21.697009Z","shell.execute_reply":"2021-08-08T08:38:21.702619Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"#modelden sonuç beklediğimiz outputu belirtiyoruz.\n\ny_data = {'decoder_output': decoder_output_data}","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:21.704462Z","iopub.execute_input":"2021-08-08T08:38:21.704746Z","iopub.status.idle":"2021-08-08T08:38:21.711970Z","shell.execute_reply.started":"2021-08-08T08:38:21.704710Z","shell.execute_reply":"2021-08-08T08:38:21.711185Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"model_train.fit(x=x_data,\n                y=y_data,\n                batch_size=256,\n                epochs=10,\n                callbacks=[checkpoint])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:38:21.713022Z","iopub.execute_input":"2021-08-08T08:38:21.713305Z","iopub.status.idle":"2021-08-08T09:22:31.261341Z","shell.execute_reply.started":"2021-08-08T08:38:21.713263Z","shell.execute_reply":"2021-08-08T09:22:31.260434Z"},"trusted":true},"execution_count":136,"outputs":[{"name":"stdout","text":"Train on 473035 samples\nEpoch 1/10\n473035/473035 [==============================] - 269s 568us/sample - loss: 3.2297\nEpoch 2/10\n473035/473035 [==============================] - 265s 561us/sample - loss: 2.2303\nEpoch 3/10\n473035/473035 [==============================] - 267s 564us/sample - loss: 1.9414\nEpoch 4/10\n473035/473035 [==============================] - 264s 559us/sample - loss: 1.8087\nEpoch 5/10\n473035/473035 [==============================] - 263s 555us/sample - loss: 1.7816\nEpoch 6/10\n473035/473035 [==============================] - 263s 555us/sample - loss: 1.7550\nEpoch 7/10\n473035/473035 [==============================] - 263s 556us/sample - loss: 1.7234\nEpoch 8/10\n473035/473035 [==============================] - 263s 556us/sample - loss: 1.6937\nEpoch 9/10\n473035/473035 [==============================] - 265s 560us/sample - loss: 1.6684\nEpoch 10/10\n473035/473035 [==============================] - 265s 561us/sample - loss: 1.6462\n","output_type":"stream"},{"execution_count":136,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f707593e5d0>"},"metadata":{}}]},{"cell_type":"code","source":"# 3 farklı model oluşturduk ama bunların weightleri aynı. model trainin encoderı ile oluşturduğumuz model encoderın weightleri aynı\n#eğitim aşamasında decodera farklı input ve output verdiğimiz için aynı modeli farklı şekillerde tanımlıyoruz.\n\n#model traine ingilizce cümleleri ve bu cümlelerin türkçe çevirileri input olarak vermiştik. modelimizi test ederken verdiğimiz ingilizce \n#cümlenin türkçe çevirisini verirsek sonucun pek anlamıyor olmuyor. model train bizden hem encoder hemde decoder için bizden input istiyor\n\n# ing cümleyi model encodera veriyoruz. bize bir düşünce vektörü verecek elde ettiğimiz düşünce vektörünü ve başlangıç tokenini \n#model decodera veriyoruz.","metadata":{"execution":{"iopub.status.busy":"2021-08-08T09:22:31.262858Z","iopub.execute_input":"2021-08-08T09:22:31.263238Z","iopub.status.idle":"2021-08-08T09:22:31.267503Z","shell.execute_reply.started":"2021-08-08T09:22:31.263198Z","shell.execute_reply":"2021-08-08T09:22:31.266552Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"def translate(input_text, true_output_text=None):\n    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n                                                reverse=True,\n                                                padding='pre')\n    \n    initial_state = model_encoder.predict(input_tokens)       # düşünce vektörünü ürettik\n    \n    max_tokens = tokenizer_dest.max_tokens \n    \n    decoder_input_data = np.zeros(shape=(1, max_tokens), dtype=np.int)  # normalde bachsize 256 ama suan 1 cümle üzerinden çalışıyoruz.\n    \n    token_int = token_start\n    output_text = ''\n    count_tokens = 0\n    \n    while token_int != token_end and count_tokens < max_tokens:\n        decoder_input_data[0, count_tokens] = token_int\n        x_data = {'decoder_initial_state': initial_state, 'decoder_input': decoder_input_data}\n        \n        decoder_output = model_decoder.predict(x_data)\n        \n        token_onehot = decoder_output[0, count_tokens, :]\n        token_int = np.argmax(token_onehot)\n        \n        sampled_word = tokenizer_dest.token_to_word(token_int)\n        output_text += ' ' + sampled_word\n        count_tokens += 1\n        \n    print('Input text:')\n    print(input_text)\n    print()\n    \n    print('Translated text:')\n    print(output_text)\n    print()\n    \n    if true_output_text is not None:\n        print('True output text:')\n        print(true_output_text)\n        print()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T09:22:31.269068Z","iopub.execute_input":"2021-08-08T09:22:31.269767Z","iopub.status.idle":"2021-08-08T09:22:31.281129Z","shell.execute_reply.started":"2021-08-08T09:22:31.269724Z","shell.execute_reply":"2021-08-08T09:22:31.280175Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"translate(input_text=data_src[400000], true_output_text=data_dest[400000])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T09:22:31.282480Z","iopub.execute_input":"2021-08-08T09:22:31.283051Z","iopub.status.idle":"2021-08-08T09:22:31.931035Z","shell.execute_reply.started":"2021-08-08T09:22:31.283008Z","shell.execute_reply":"2021-08-08T09:22:31.930228Z"},"trusted":true},"execution_count":139,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n  warnings.warn('`Model.state_updates` will be removed in a future version. '\n","output_type":"stream"},{"name":"stdout","text":"Input text:\nYou are telling it second hand, aren't you?\n\nTranslated text:\n onu ikinci şekilde söylüyorsun değil mi eeee\n\nTrue output text:\nssss Onu dolaylı olarak anlatıyorsun, değil mi? eeee\n\n","output_type":"stream"}]},{"cell_type":"code","source":"translate(input_text=data_src[300000], true_output_text=data_dest[300000])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T09:26:36.811738Z","iopub.execute_input":"2021-08-08T09:26:36.812062Z","iopub.status.idle":"2021-08-08T09:26:36.895351Z","shell.execute_reply.started":"2021-08-08T09:26:36.812033Z","shell.execute_reply":"2021-08-08T09:26:36.894520Z"},"trusted":true},"execution_count":148,"outputs":[{"name":"stdout","text":"Input text:\nTom will certainly try to do that.\n\nTranslated text:\n tom kesinlikle bunu yapmaya çalışacak eeee\n\nTrue output text:\nssss Tom kesinlikle onu yapmaya çalışacak. eeee\n\n","output_type":"stream"}]},{"cell_type":"code","source":"translate(input_text=data_src[2000], true_output_text=data_dest[2000])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T09:26:51.068377Z","iopub.execute_input":"2021-08-08T09:26:51.068708Z","iopub.status.idle":"2021-08-08T09:26:51.127182Z","shell.execute_reply.started":"2021-08-08T09:26:51.068675Z","shell.execute_reply":"2021-08-08T09:26:51.126361Z"},"trusted":true},"execution_count":149,"outputs":[{"name":"stdout","text":"Input text:\nI feel weak.\n\nTranslated text:\n ben zayıf hissediyorum eeee\n\nTrue output text:\nssss Cılız hissediyorum. eeee\n\n","output_type":"stream"}]},{"cell_type":"code","source":"translate(input_text='Why people are so rude')","metadata":{"execution":{"iopub.status.busy":"2021-08-08T09:22:31.932316Z","iopub.execute_input":"2021-08-08T09:22:31.932645Z","iopub.status.idle":"2021-08-08T09:22:32.013818Z","shell.execute_reply.started":"2021-08-08T09:22:31.932609Z","shell.execute_reply":"2021-08-08T09:22:32.012891Z"},"trusted":true},"execution_count":140,"outputs":[{"name":"stdout","text":"Input text:\nWhy people are so rude\n\nTranslated text:\n i̇nsanlar neden bu kadar kaba eeee\n\n","output_type":"stream"}]},{"cell_type":"code","source":"translate(input_text='This summer ı went to Turkey')","metadata":{"execution":{"iopub.status.busy":"2021-08-08T09:26:15.215987Z","iopub.execute_input":"2021-08-08T09:26:15.216341Z","iopub.status.idle":"2021-08-08T09:26:15.298627Z","shell.execute_reply.started":"2021-08-08T09:26:15.216308Z","shell.execute_reply":"2021-08-08T09:26:15.297746Z"},"trusted":true},"execution_count":147,"outputs":[{"name":"stdout","text":"Input text:\nThis summer ı went to cinema\n\nTranslated text:\n bu yaz yaz tom'un çıktı eeee\n\n","output_type":"stream"}]},{"cell_type":"code","source":"translate(input_text='Where did you buy this dress?')","metadata":{"execution":{"iopub.status.busy":"2021-08-08T09:25:55.143241Z","iopub.execute_input":"2021-08-08T09:25:55.143563Z","iopub.status.idle":"2021-08-08T09:25:55.208589Z","shell.execute_reply.started":"2021-08-08T09:25:55.143535Z","shell.execute_reply":"2021-08-08T09:25:55.207747Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"Input text:\nWhere did you buy this dress?\n\nTranslated text:\n bu elbiseyi nereden aldın eeee\n\n","output_type":"stream"}]},{"cell_type":"code","source":"translate(input_text='which road leads to airport')","metadata":{"execution":{"iopub.status.busy":"2021-08-08T09:25:58.255877Z","iopub.execute_input":"2021-08-08T09:25:58.256193Z","iopub.status.idle":"2021-08-08T09:25:58.323113Z","shell.execute_reply.started":"2021-08-08T09:25:58.256164Z","shell.execute_reply":"2021-08-08T09:25:58.322319Z"},"trusted":true},"execution_count":145,"outputs":[{"name":"stdout","text":"Input text:\nwhich road leads to airport\n\nTranslated text:\n hangi yol havaalanına gider eeee\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}