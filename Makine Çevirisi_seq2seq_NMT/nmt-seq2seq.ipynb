{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-03T06:44:29.913370Z","iopub.execute_input":"2021-08-03T06:44:29.913730Z","iopub.status.idle":"2021-08-03T06:44:29.926465Z","shell.execute_reply.started":"2021-08-03T06:44:29.913698Z","shell.execute_reply":"2021-08-03T06:44:29.925644Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"/kaggle/input/glove6b/glove.6B.200d.txt\n/kaggle/input/glove6b/glove.6B.50d.txt\n/kaggle/input/glove6b/glove.6B.300d.txt\n/kaggle/input/glove6b/glove.6B.100d.txt\n/kaggle/input/turkish-to-english-translator/TR2EN.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"data seti 450k dan oluşan bir dataset","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:44:29.928292Z","iopub.execute_input":"2021-08-03T06:44:29.928643Z","iopub.status.idle":"2021-08-03T06:44:29.935384Z","shell.execute_reply.started":"2021-08-03T06:44:29.928610Z","shell.execute_reply":"2021-08-03T06:44:29.934484Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.keras.models import Model\nfrom tensorflow.python.keras.layers import Input, Dense, GRU, Embedding, CuDNNGRU\nfrom keras.optimizers import RMSprop\nfrom tensorflow.python.keras.callbacks import ModelCheckpoint\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:44:29.937113Z","iopub.execute_input":"2021-08-03T06:44:29.937588Z","iopub.status.idle":"2021-08-03T06:44:29.944036Z","shell.execute_reply.started":"2021-08-03T06:44:29.937431Z","shell.execute_reply":"2021-08-03T06:44:29.943048Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"#model cümle üretmeye başlangıç tokenini gördükten sonra başlıyor. başlangıç tokeni datasette olmayan bir kelime olmalıdır.\n#cümlenin sonlanması için bitiş tokenini belirtmemiz gerekiyor.\n#boşlum karekterleri çok önemli !!!!\nmark_start = 'ssss '\nmark_end = ' eeee'","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:44:29.945687Z","iopub.execute_input":"2021-08-03T06:44:29.946049Z","iopub.status.idle":"2021-08-03T06:44:29.952218Z","shell.execute_reply.started":"2021-08-03T06:44:29.946015Z","shell.execute_reply":"2021-08-03T06:44:29.951430Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"data_src = []\ndata_dest = []","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:44:29.953554Z","iopub.execute_input":"2021-08-03T06:44:29.954177Z","iopub.status.idle":"2021-08-03T06:44:29.990902Z","shell.execute_reply.started":"2021-08-03T06:44:29.954141Z","shell.execute_reply":"2021-08-03T06:44:29.990156Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"for line in open('../input/turkish-to-english-translator/TR2EN.txt', encoding='UTF-8'):\n    en_text, tr_text = line.rstrip().split('\\t')\n    tr_text = mark_start + tr_text + mark_end\n    data_src.append(en_text)\n    data_dest.append(tr_text)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:44:29.992145Z","iopub.execute_input":"2021-08-03T06:44:29.992543Z","iopub.status.idle":"2021-08-03T06:44:30.741445Z","shell.execute_reply.started":"2021-08-03T06:44:29.992505Z","shell.execute_reply":"2021-08-03T06:44:30.740590Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"print(data_src[100])\nprint(data_dest[100])\nprint(data_src[200000])\nprint(data_dest[200000])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:44:30.744140Z","iopub.execute_input":"2021-08-03T06:44:30.744537Z","iopub.status.idle":"2021-08-03T06:44:30.750088Z","shell.execute_reply.started":"2021-08-03T06:44:30.744500Z","shell.execute_reply":"2021-08-03T06:44:30.749265Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"I drove.\nssss Araba sürdüm. eeee\nCan you see anything missing?\nssss Eksik bir şey görebiliyor musun? eeee\n","output_type":"stream"}]},{"cell_type":"code","source":"class TokenizerWrap(Tokenizer):\n    def __init__(self, texts, padding, reverse=False, num_words=None):\n        Tokenizer.__init__(self, num_words=num_words) #tokenizer tanımladık\n        \n        self.fit_on_texts(texts) # aldığımız texti tokenleştirdik\n        \n        # tokenleri text haline getirmek için key ve valueların yerini değiştiriyoruz.\n        #value kelimeler key ise bu kelimeleri temsil eden sayılardan oluşuyor\n        #bu sayede tokenları tekrar stringe çevirebiliriz.\n        self.index_to_word = dict(zip(self.word_index.values(), self.word_index.keys()))\n        \n        self.tokens = self.texts_to_sequences(texts) # elimizdeki yazıları tokenler olarak bir listede topluyoruz.\n        \n        #diyelimki inputumuz 30 tokendan oluşuyor ama biz inputu 20 ile sınırladık 10 tanesini kesiyoruz buna truncating deniliyor.\n        #pre ile baştan 10 token post diyince sondan 10 token atılıyor.\n        \n        if reverse:\n            self.tokens = [list(reversed(x)) for x in self.tokens]\n            truncating = 'pre'\n        else:\n            truncating = 'post'\n        \n\n            \n        self.num_tokens = [len(x) for x in self.tokens]\n        self.max_tokens = np.mean(self.num_tokens) + 2 * np.std(self.num_tokens)\n        self.max_tokens = int(self.max_tokens)\n        # böylece elimizdeki inputların boyutuna karar veriyoruz.\n        \n        \n        self.tokens_padded = pad_sequences(self.tokens,\n                                           maxlen=self.max_tokens,\n                                           padding=padding,\n                                           truncating=truncating)\n        \n    def token_to_word(self, token): # bir sayıya denk gelen kelimeyi döndürüyoruz. \n        word = ' ' if token == 0 else self.index_to_word[token]\n        return word\n    \n    def tokens_to_string(self, tokens): # token listesini cümleye çeviriyoruz. eğer token sıfır değilse çünkü paddingte sıfır eklemiştik\n        words = [self.index_to_word[token] for token in tokens if token != 0]\n        text = ' '.join(words)\n        return text\n    \n    def text_to_tokens(self, text, padding, reverse=False): # modele cümleyi token olarak vermek için hazırlıyoruz.\n        tokens = self.texts_to_sequences([text]) # önce aldığımız texti tokenleştiriyoruz.\n        tokens = np.array(tokens) # daha sonra bu tokenleri numpy arraye çeviriyoruz.\n        \n        if reverse:\n            tokens = np.flip(tokens, axis=1) #satır üzerinde ters çevirme yapıyoruz.\n            truncating = 'pre'\n        else:\n            truncating = 'post'\n            \n        tokens = pad_sequences(tokens,\n                               maxlen=self.max_tokens,\n                               padding=padding,\n                               truncating=truncating)\n        \n        return tokens","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:44:30.751891Z","iopub.execute_input":"2021-08-03T06:44:30.752449Z","iopub.status.idle":"2021-08-03T06:44:30.765644Z","shell.execute_reply.started":"2021-08-03T06:44:30.752414Z","shell.execute_reply":"2021-08-03T06:44:30.764766Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"tokenizer_src = TokenizerWrap(texts=data_src,\n                              padding='pre',\n                              reverse=True,\n                              num_words=None)\n\n#reverse true olunca kelimeler yer değiştiriyor. en başta ki kelime en sona geçiyordu.\n#mesela bugün hava çok soğuk bu cümlenin boyutu 4, biz eğer kelimenin boyutu 3 olsun istiyorsak truncatinge pre verirsek cümle başındaki\n#bugün kelimesi atılacaktır. ama biz öncesinde reverse ediyoruz.-> soğuk çok hava bugün oluyor bu şekilde pre truncating uyguladığımızda \n#cümlenin başını korumuş oluyoruz.","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:44:30.766838Z","iopub.execute_input":"2021-08-03T06:44:30.767324Z","iopub.status.idle":"2021-08-03T06:44:45.353507Z","shell.execute_reply.started":"2021-08-03T06:44:30.767289Z","shell.execute_reply":"2021-08-03T06:44:45.352503Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"tokenizer_dest = TokenizerWrap(texts=data_dest,\n                              padding='post',\n                              reverse=False,\n                              num_words=None)\n\n#pre padding yapıyoruz encoderda önce sıfırları görüyor kelimeden önce. network en son gördüğü şeyleri daha iyi aklında tutacağı için pre padding yapıyoruz\n#düşünce vektörü üretmeden hemen önce kelimeleri gördüğü için ingilizce cümleyi daha iyi yansıttığından dolayı encoderda pre padding kullanmamızın temel sebebi bu\n\n#düşünce vektörü üretildikten sonra bu vektör decodera veriliyor. decoderın kelime vektörünü üretmesi bekleniyor.\n#decoderın düşünce vektörünü görür görmez kelime vektörünü üretmesini isteris sıfırlarla uğraşmasını istemeyiz bu yüzden post padding yapıyoruz.\n#post padding ile cümlenin sonuna sıfırlar ekleniyor. bu tokenlerin modele etkisinin olmasını istemeyiz. eğer pre padding uygulasaydık düşünce\n#vektörünü aldıktan sonra decoder bir sürü sıfır görüp buna göre eğitim yapıcaktı post padding ile bunu engelliyoruz. ve direk\n#kelimeyi gösteriyoruz.. reverse false verince truncating post oluyordu, cümledeki kelimelerin yerleri ile oynanmıyordu. \n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:44:45.354841Z","iopub.execute_input":"2021-08-03T06:44:45.355215Z","iopub.status.idle":"2021-08-03T06:45:03.425829Z","shell.execute_reply.started":"2021-08-03T06:44:45.355162Z","shell.execute_reply":"2021-08-03T06:45:03.424985Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"tokens_src = tokenizer_src.tokens_padded\ntokens_dest = tokenizer_dest.tokens_padded\nprint(tokens_src.shape) #ingilizcede cümle boyutumuz 11\nprint(tokens_dest.shape) #türkçede cümle boyutumuz 10","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.427079Z","iopub.execute_input":"2021-08-03T06:45:03.427429Z","iopub.status.idle":"2021-08-03T06:45:03.432121Z","shell.execute_reply.started":"2021-08-03T06:45:03.427396Z","shell.execute_reply":"2021-08-03T06:45:03.431340Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"(473035, 11)\n(473035, 10)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tokens_dest[200000])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.433554Z","iopub.execute_input":"2021-08-03T06:45:03.434026Z","iopub.status.idle":"2021-08-03T06:45:03.446703Z","shell.execute_reply.started":"2021-08-03T06:45:03.433992Z","shell.execute_reply":"2021-08-03T06:45:03.445782Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"[   1 2391    4   18 4127   48    2    0    0    0]\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer_dest.tokens_to_string(tokens_dest[200000])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.448132Z","iopub.execute_input":"2021-08-03T06:45:03.448571Z","iopub.status.idle":"2021-08-03T06:45:03.456585Z","shell.execute_reply.started":"2021-08-03T06:45:03.448538Z","shell.execute_reply":"2021-08-03T06:45:03.455728Z"},"trusted":true},"execution_count":100,"outputs":[{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"'ssss eksik bir şey görebiliyor musun eeee'"},"metadata":{}}]},{"cell_type":"code","source":"print(tokens_src[200000])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.457889Z","iopub.execute_input":"2021-08-03T06:45:03.458375Z","iopub.status.idle":"2021-08-03T06:45:03.465116Z","shell.execute_reply.started":"2021-08-03T06:45:03.458339Z","shell.execute_reply":"2021-08-03T06:45:03.464195Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"[   0    0    0    0    0    0 1028  113   95    5   39]\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer_src.tokens_to_string(tokens_src[200000])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.466516Z","iopub.execute_input":"2021-08-03T06:45:03.466856Z","iopub.status.idle":"2021-08-03T06:45:03.474404Z","shell.execute_reply.started":"2021-08-03T06:45:03.466824Z","shell.execute_reply":"2021-08-03T06:45:03.473357Z"},"trusted":true},"execution_count":102,"outputs":[{"execution_count":102,"output_type":"execute_result","data":{"text/plain":"'missing anything see you can'"},"metadata":{}}]},{"cell_type":"markdown","source":"# ÖNEMLİ AÇIKLAMA ","metadata":{}},{"cell_type":"markdown","source":"> Encodera vereceğimiz inputu tokenleştirme yaparak neden ters çeviriyoruz ?\n> \n> bunun iki türlü faydası var:\n>     birincisi truncating yaparken cümleyi başından kesiyoruz. eğer cümleyi ters çevirmezsek cümleyi kesince baştaki kelimeler gidecekti. \n>     \n>     bunun şöyle bir yararı var:\n>         encodera verdiğimiz inputta baştaki kelimelerin kesildiğini düşünelim. encoder çalışıp düşünce vektörü oluştuurp decodera verecek\n>         decoderda ise truncatingi post yaptık yani cümlenin sonundaki kelimeler kesiliyor. öyleyse encodera verilecek input bir cümlenin \n>         sonundaki kelimelerden oluşacak, decoderda ise bir cümlenin başındaki kelimelerden oluşacak. yani network ingilizce cümlelerin \n>         sonu   ile türkçe cümlelerin başını eşleştirecek bu şekilde doğru kelimeler birbirleri ile eşleşecek.\n>         \n>     ikinci avantajı:\n>         cümleleri encodera verince en sondan başlayarak başa doğru ilerleyecek encoder düşünce vektörü üretmeden önce gördüğü son şey\n>         cümlenin başı olacak. decodera geçtiğinde ise direk cümlenin başından başlayıp üretmeye başlayacak bu şekilde en son cümlenin başını \n>         görüp üretmeye başlarsa daha isabetli bir çeviri yapabilir.\n>     ","metadata":{}},{"cell_type":"code","source":"data_src[200000]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.475764Z","iopub.execute_input":"2021-08-03T06:45:03.476109Z","iopub.status.idle":"2021-08-03T06:45:03.483398Z","shell.execute_reply.started":"2021-08-03T06:45:03.476077Z","shell.execute_reply":"2021-08-03T06:45:03.482373Z"},"trusted":true},"execution_count":103,"outputs":[{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"'Can you see anything missing?'"},"metadata":{}}]},{"cell_type":"code","source":"token_start = tokenizer_dest.word_index[mark_start.strip()] #word_indexte kelimelerin tokenleri bulunuyor.\ntoken_start","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.490435Z","iopub.execute_input":"2021-08-03T06:45:03.490670Z","iopub.status.idle":"2021-08-03T06:45:03.495822Z","shell.execute_reply.started":"2021-08-03T06:45:03.490647Z","shell.execute_reply":"2021-08-03T06:45:03.494864Z"},"trusted":true},"execution_count":104,"outputs":[{"execution_count":104,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"token_end = tokenizer_dest.word_index[mark_end.strip()]\ntoken_end","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.498312Z","iopub.execute_input":"2021-08-03T06:45:03.498953Z","iopub.status.idle":"2021-08-03T06:45:03.505215Z","shell.execute_reply.started":"2021-08-03T06:45:03.498916Z","shell.execute_reply":"2021-08-03T06:45:03.504247Z"},"trusted":true},"execution_count":105,"outputs":[{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"encoder_input_data = tokens_src \nencoder_input_data # verisetindeki cümlelerin token listesi","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.506438Z","iopub.execute_input":"2021-08-03T06:45:03.507126Z","iopub.status.idle":"2021-08-03T06:45:03.514911Z","shell.execute_reply.started":"2021-08-03T06:45:03.507051Z","shell.execute_reply":"2021-08-03T06:45:03.513888Z"},"trusted":true},"execution_count":106,"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"array([[   0,    0,    0, ...,    0,    0, 2736],\n       [   0,    0,    0, ...,    0,    0, 2736],\n       [   0,    0,    0, ...,    0,    0,  480],\n       ...,\n       [   4, 7097,   49, ..., 1641,  461, 1608],\n       [ 178,    5, 1323, ...,   31,    5,   72],\n       [ 682,    5,    8, ...,   81,  304,   72]], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"decoder_input_data = tokens_dest[:, :-1] # baştan başla sondan bir eksiğe kadar git\ndecoder_output_data = tokens_dest[:, 1:] # 1 den başla sona kadar git.","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.516243Z","iopub.execute_input":"2021-08-03T06:45:03.516600Z","iopub.status.idle":"2021-08-03T06:45:03.522267Z","shell.execute_reply.started":"2021-08-03T06:45:03.516565Z","shell.execute_reply":"2021-08-03T06:45:03.521238Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"encoder_input_data[200000]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.523718Z","iopub.execute_input":"2021-08-03T06:45:03.524130Z","iopub.status.idle":"2021-08-03T06:45:03.535863Z","shell.execute_reply.started":"2021-08-03T06:45:03.524098Z","shell.execute_reply":"2021-08-03T06:45:03.534847Z"},"trusted":true},"execution_count":108,"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"array([   0,    0,    0,    0,    0,    0, 1028,  113,   95,    5,   39],\n      dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"decoder_input_data[200000]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.537319Z","iopub.execute_input":"2021-08-03T06:45:03.537733Z","iopub.status.idle":"2021-08-03T06:45:03.545615Z","shell.execute_reply.started":"2021-08-03T06:45:03.537697Z","shell.execute_reply":"2021-08-03T06:45:03.544452Z"},"trusted":true},"execution_count":109,"outputs":[{"execution_count":109,"output_type":"execute_result","data":{"text/plain":"array([   1, 2391,    4,   18, 4127,   48,    2,    0,    0], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"decoder_output_data[200000]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.547086Z","iopub.execute_input":"2021-08-03T06:45:03.547511Z","iopub.status.idle":"2021-08-03T06:45:03.555039Z","shell.execute_reply.started":"2021-08-03T06:45:03.547477Z","shell.execute_reply":"2021-08-03T06:45:03.554033Z"},"trusted":true},"execution_count":110,"outputs":[{"execution_count":110,"output_type":"execute_result","data":{"text/plain":"array([2391,    4,   18, 4127,   48,    2,    0,    0,    0], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"aşağıda görüldüğü üzere input ve outputta tek fark outputta başlangıç tokeni yok\n\ndecoderda inputtan alınan başlangıç tokeniyle eksik kelimesinin üretilmesi beklenir, daha sonra eksik kelimesi input olduğunda \nbir kelimesinin outputta olması gerekir bu olay bitiş tokenine kadar böyle devam eder","metadata":{}},{"cell_type":"code","source":"tokenizer_dest.tokens_to_string(decoder_input_data[200000])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.556470Z","iopub.execute_input":"2021-08-03T06:45:03.556889Z","iopub.status.idle":"2021-08-03T06:45:03.564036Z","shell.execute_reply.started":"2021-08-03T06:45:03.556846Z","shell.execute_reply":"2021-08-03T06:45:03.563010Z"},"trusted":true},"execution_count":111,"outputs":[{"execution_count":111,"output_type":"execute_result","data":{"text/plain":"'ssss eksik bir şey görebiliyor musun eeee'"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer_dest.tokens_to_string(decoder_output_data[200000])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.565447Z","iopub.execute_input":"2021-08-03T06:45:03.565935Z","iopub.status.idle":"2021-08-03T06:45:03.572721Z","shell.execute_reply.started":"2021-08-03T06:45:03.565900Z","shell.execute_reply":"2021-08-03T06:45:03.571614Z"},"trusted":true},"execution_count":112,"outputs":[{"execution_count":112,"output_type":"execute_result","data":{"text/plain":"'eksik bir şey görebiliyor musun eeee'"},"metadata":{}}]},{"cell_type":"code","source":"num_encoder_words = len(tokenizer_src.word_index)\nnum_decoder_words = len(tokenizer_dest.word_index)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.574393Z","iopub.execute_input":"2021-08-03T06:45:03.574803Z","iopub.status.idle":"2021-08-03T06:45:03.579435Z","shell.execute_reply.started":"2021-08-03T06:45:03.574711Z","shell.execute_reply":"2021-08-03T06:45:03.578438Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"print(num_encoder_words)\nprint(num_decoder_words)\n\n#türkçe eklemeli bir dil olduğu için  kelime sayısı daha çok çıktı ","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.581282Z","iopub.execute_input":"2021-08-03T06:45:03.581673Z","iopub.status.idle":"2021-08-03T06:45:03.588584Z","shell.execute_reply.started":"2021-08-03T06:45:03.581587Z","shell.execute_reply":"2021-08-03T06:45:03.587509Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stdout","text":"21315\n94058\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ENCODER","metadata":{}},{"cell_type":"markdown","source":"**Embedding layerda kelime vektörlerini oluşturuken GLOVE kullanacağız.**","metadata":{}},{"cell_type":"markdown","source":"elimizde bulunan 21k lık kelimelere karşılık vektör bulamayabiliriz. tüm kelime vektörlerini rastgele tanımlıyoruz.\neğer glove kelime vektörleri içerisinde kelime haznemizdeki kelimelerin vektörleri bulunuyorsa o vektör rastgele oluşturulan vektörün yerine geçicek.\n\n400.000 kelimenin bulunduğu glove vektörünün içinde bizim kelimelerimiz bulunacaktır.","metadata":{}},{"cell_type":"code","source":"embedding_size = 100   # glove vektörlerinin uzunluğuda 100 ","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.590335Z","iopub.execute_input":"2021-08-03T06:45:03.590892Z","iopub.status.idle":"2021-08-03T06:45:03.595455Z","shell.execute_reply.started":"2021-08-03T06:45:03.590858Z","shell.execute_reply":"2021-08-03T06:45:03.594348Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"**önce boş bir sözlük oluşturuyoruz, -->> word2vec={}**\n\n**daha sonra kelime ve vektörü olacak şekilde içini dolduruyoruz.**","metadata":{}},{"cell_type":"code","source":"word2vec = {}\nwith open('../input/glove6b/glove.6B.100d.txt', encoding='UTF-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:03.597138Z","iopub.execute_input":"2021-08-03T06:45:03.597570Z","iopub.status.idle":"2021-08-03T06:45:19.316263Z","shell.execute_reply.started":"2021-08-03T06:45:03.597537Z","shell.execute_reply":"2021-08-03T06:45:19.315404Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"#burada yaptığım işlem embedding vektörün içini dolduruyorum eğer kelime glove vektörünün içinde varsa o vektörü alıyorum\n\nembedding_matrix = np.random.uniform(-1, 1, (num_encoder_words, embedding_size))\nfor word, i in tokenizer_src.word_index.items():\n    if i < num_encoder_words:\n        embedding_vector = word2vec.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:19.317616Z","iopub.execute_input":"2021-08-03T06:45:19.317941Z","iopub.status.idle":"2021-08-03T06:45:19.381009Z","shell.execute_reply.started":"2021-08-03T06:45:19.317907Z","shell.execute_reply":"2021-08-03T06:45:19.380267Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"embedding_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:19.382305Z","iopub.execute_input":"2021-08-03T06:45:19.382793Z","iopub.status.idle":"2021-08-03T06:45:19.388625Z","shell.execute_reply.started":"2021-08-03T06:45:19.382755Z","shell.execute_reply":"2021-08-03T06:45:19.387592Z"},"trusted":true},"execution_count":118,"outputs":[{"execution_count":118,"output_type":"execute_result","data":{"text/plain":"(21315, 100)"},"metadata":{}}]},{"cell_type":"code","source":"encoder_input = Input(shape=(None,), name='encoder_input') #shape none vererek boyutla ilgili sorunları çözüyoruz","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:19.390335Z","iopub.execute_input":"2021-08-03T06:45:19.390675Z","iopub.status.idle":"2021-08-03T06:45:19.398081Z","shell.execute_reply.started":"2021-08-03T06:45:19.390641Z","shell.execute_reply":"2021-08-03T06:45:19.397267Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"#kerasın embedding layerına ekliyoruz.\nencoder_embedding = Embedding(input_dim=num_encoder_words, \n                              output_dim=embedding_size,\n                              weights=[embedding_matrix],\n                              trainable=True,\n                              name='encoder_embedding')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:19.399610Z","iopub.execute_input":"2021-08-03T06:45:19.400058Z","iopub.status.idle":"2021-08-03T06:45:19.407295Z","shell.execute_reply.started":"2021-08-03T06:45:19.400023Z","shell.execute_reply":"2021-08-03T06:45:19.406464Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"state_size = 256 # butün GRU layerlar 256 boyutlu output üretecek. daha yüksek bir state_size ile daha iyi sonuç alınır.","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:19.408582Z","iopub.execute_input":"2021-08-03T06:45:19.408990Z","iopub.status.idle":"2021-08-03T06:45:19.414100Z","shell.execute_reply.started":"2021-08-03T06:45:19.408957Z","shell.execute_reply":"2021-08-03T06:45:19.413322Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"#3 katmanlı bir GRU yapısı oluşturuyoruz.\nencoder_gru1 = GRU(state_size, name='encoder_gru1', return_sequences=True)\nencoder_gru2 = GRU(state_size, name='encoder_gru2', return_sequences=True)\nencoder_gru3 = GRU(state_size, name='encoder_gru3', return_sequences=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:19.415263Z","iopub.execute_input":"2021-08-03T06:45:19.415770Z","iopub.status.idle":"2021-08-03T06:45:19.431484Z","shell.execute_reply.started":"2021-08-03T06:45:19.415737Z","shell.execute_reply":"2021-08-03T06:45:19.430701Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"#layerları connect_encoder()' da görüldüğü gibi bağlıyoruz.\ndef connect_encoder():\n    net = encoder_input\n    \n    net = encoder_embedding(net)\n    \n    net = encoder_gru1(net)\n    net = encoder_gru2(net)\n    net = encoder_gru3(net)\n    \n    encoder_output = net\n    \n    return encoder_output\n\n\n# encoder_output ile düşünce vektörünü oluşturmuş olduk . artık bu fonk çağırarak düşünce vektörüne ulaşabiliriz.","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:19.432730Z","iopub.execute_input":"2021-08-03T06:45:19.433234Z","iopub.status.idle":"2021-08-03T06:45:19.442539Z","shell.execute_reply.started":"2021-08-03T06:45:19.433179Z","shell.execute_reply":"2021-08-03T06:45:19.441706Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"encoder_output = connect_encoder()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:19.443735Z","iopub.execute_input":"2021-08-03T06:45:19.444080Z","iopub.status.idle":"2021-08-03T06:45:19.884143Z","shell.execute_reply.started":"2021-08-03T06:45:19.444046Z","shell.execute_reply":"2021-08-03T06:45:19.883350Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"markdown","source":"# DECODER","metadata":{}},{"cell_type":"markdown","source":"Encodera ingilizce cümleleri Decodera bu ingilizce cümlelerin türkçe çevirilerini veriyoruz.\n\neğitimde decodera düşünce vektörünü ve ingilizce cümleye karşılık gelen türkçe cümleyi veriyoruz ve modeli eğitiyoruz.\n\neğitim tamamlandıktan sonra encodera verdiğimiz ing cümlenin sonuda aldığımız düşünce vektörünü başlanğıç tokeni ile birlikte decodera vereceğiz.","metadata":{}},{"cell_type":"code","source":"decoder_initial_state = Input(shape=(state_size,), name='decoder_initial_state')\n    \n# decodera vereceğimiz düşünce vektörü için bir input layer oluştuuryoruz.","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:19.886127Z","iopub.execute_input":"2021-08-03T06:45:19.886741Z","iopub.status.idle":"2021-08-03T06:45:19.892386Z","shell.execute_reply.started":"2021-08-03T06:45:19.886692Z","shell.execute_reply":"2021-08-03T06:45:19.891585Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"decoder_input = Input(shape=(None,), name='decoder_input') \n\n# bu input ise decodera vereceğimiz türkçe cümle için, shape ' e none veriyoruz gelen her input kabulümüz.","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:19.893577Z","iopub.execute_input":"2021-08-03T06:45:19.893920Z","iopub.status.idle":"2021-08-03T06:45:19.904852Z","shell.execute_reply.started":"2021-08-03T06:45:19.893886Z","shell.execute_reply":"2021-08-03T06:45:19.904049Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"#normalde glove ile embedding matrisnini oluşturuyoduk ama elimizdeki glove datası ing için bizim decoderımızda türkçe veriler var\n#ya yeni bir glove modeli eğiticez yada yaptığım gibi rastgele dolduracağız.\n\ndecoder_embedding = Embedding(input_dim=num_decoder_words,\n                              output_dim=embedding_size,\n                              name='decoder_embedding')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:19.906982Z","iopub.execute_input":"2021-08-03T06:45:19.907625Z","iopub.status.idle":"2021-08-03T06:45:19.914855Z","shell.execute_reply.started":"2021-08-03T06:45:19.907588Z","shell.execute_reply":"2021-08-03T06:45:19.914092Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"decoder_gru1 = GRU(state_size, name='decoder_gru1', return_sequences=True)\ndecoder_gru2 = GRU(state_size, name='decoder_gru2', return_sequences=True)\ndecoder_gru3 = GRU(state_size, name='decoder_gru3', return_sequences=True)\n\n# decoderda üçüncü layera true dedik çünkü buradan bize bir cümle dönüyor, encoderda bize bir vektör dönüyordu.","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:19.916061Z","iopub.execute_input":"2021-08-03T06:45:19.916425Z","iopub.status.idle":"2021-08-03T06:45:19.935342Z","shell.execute_reply.started":"2021-08-03T06:45:19.916389Z","shell.execute_reply":"2021-08-03T06:45:19.934503Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"# decoderın çalışması sonucunda kelimeler elde etmek istiyoruz. suan layerın çalışması durumunda 256 boyutunda vektörler elde edeceğiz.\n# bu vektörleri kelimeye çevirmemiz lazım bunun için dense layer oluşturuyoruz. GRUdan dönen sonucu one hot türünde arraye dönüştüreceğiz.\n# bu vektörde en büyük değere sahip elemanın indexi output olarak hangi kelimenin verilemsi gerektiğini gösteriyor.\n\n#aktvasyonu linear yapınca değerler olduğu gibi geçiyor.\n\n\ndecoder_dense = Dense(num_decoder_words,\n                      activation='linear',\n                      name='decoder_output')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:19.939218Z","iopub.execute_input":"2021-08-03T06:45:19.939773Z","iopub.status.idle":"2021-08-03T06:45:19.951442Z","shell.execute_reply.started":"2021-08-03T06:45:19.939735Z","shell.execute_reply":"2021-08-03T06:45:19.950285Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"#initial_state encoderın ürettiği düşünce vektörünü veriyoruz.\n\ndef connect_decoder(initial_state):\n    net = decoder_input\n    \n    net = decoder_embedding(net)\n    \n    net = decoder_gru1(net, initial_state=initial_state)\n    net = decoder_gru2(net, initial_state=initial_state)\n    net = decoder_gru3(net, initial_state=initial_state)\n    \n    decoder_output = decoder_dense(net)\n    \n    return decoder_output","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:19.955972Z","iopub.execute_input":"2021-08-03T06:45:19.956525Z","iopub.status.idle":"2021-08-03T06:45:19.967789Z","shell.execute_reply.started":"2021-08-03T06:45:19.956492Z","shell.execute_reply":"2021-08-03T06:45:19.966876Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"decoder_output = connect_decoder(initial_state=encoder_output)  # encoder_outputu (düşünce vektörünü ) -> decodera veriyoruz.\n\nmodel_train = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output]) \n\n#decodera iki farklı input veriyoruz bunlardan ilki düşünce vektörü diğeri türkçe cümle.\n#model_train ile encoder ve decoder birbirleri ile bağlanmış oldu.","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:19.969834Z","iopub.execute_input":"2021-08-03T06:45:19.970249Z","iopub.status.idle":"2021-08-03T06:45:20.513206Z","shell.execute_reply.started":"2021-08-03T06:45:19.970214Z","shell.execute_reply":"2021-08-03T06:45:20.512373Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"model_encoder = Model(inputs=[encoder_input], outputs=[encoder_output])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:20.515017Z","iopub.execute_input":"2021-08-03T06:45:20.515520Z","iopub.status.idle":"2021-08-03T06:45:20.522802Z","shell.execute_reply.started":"2021-08-03T06:45:20.515485Z","shell.execute_reply":"2021-08-03T06:45:20.521911Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"# iki farklı model daha tanımlayacağız bunları ise model \n# tamamlandıktan sonra çeviri için kullancağız. \n#encoder için ayrı decoder için ayrı bir sonuç elde edeceğiz.\n\ndecoder_output = connect_decoder(initial_state=decoder_initial_state)\n\nmodel_decoder = Model(inputs=[decoder_input, decoder_initial_state], outputs=[decoder_output])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:20.524007Z","iopub.execute_input":"2021-08-03T06:45:20.524395Z","iopub.status.idle":"2021-08-03T06:45:20.829721Z","shell.execute_reply.started":"2021-08-03T06:45:20.524356Z","shell.execute_reply":"2021-08-03T06:45:20.828925Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"#üretilen türkçe kelimeyi gerçekteki türkçe kelime ile karşılaştırmalıyız \n#şimdi her kelime için one hot vektör tanımlayıp bunun karşılaştırılması yapılabilir ama inanılmaz büyük bir boyuta sahip olucak\n# bu yüzden de bunu kullanmak yerine spars cross entropy kullanıyoruz.\n\n# sparse cross entropy --> her iterasyonda kendi içerisinde intleri one hot vektörleri çeviriyor bu sayede kocaman bir matris oluşturmuyoruz.\n\n# spars entropyde softmaxten geçiyor bu yüzden decoderda dense layerda softmax kullanmadık. lineer kullandık\n\ndef sparse_cross_entropy(y_true, y_pred):\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n    loss_mean = tf.reduce_mean(loss)\n    return loss_mean\n","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:20.834144Z","iopub.execute_input":"2021-08-03T06:45:20.834418Z","iopub.status.idle":"2021-08-03T06:45:20.840739Z","shell.execute_reply.started":"2021-08-03T06:45:20.834392Z","shell.execute_reply":"2021-08-03T06:45:20.839918Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"# daha önce optimizasyon algoritması için adam optimizasyon algo kullanmıştım şimdi RMS kullanıyorum. RNNlerde daha iyi sonuç veren bir\n# op. algoritması.\n\noptimizer = RMSprop(lr=1e-3)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:20.842089Z","iopub.execute_input":"2021-08-03T06:45:20.842486Z","iopub.status.idle":"2021-08-03T06:45:20.851918Z","shell.execute_reply.started":"2021-08-03T06:45:20.842457Z","shell.execute_reply":"2021-08-03T06:45:20.851085Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\n\n#import tensorflow as tf\n#tf.compat.v1.disable_eager_execution()\n\ntf.disable_v2_behavior()\n#x = tf.placeholder(shape=[None, None], dtype=tf.int32)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:20.853257Z","iopub.execute_input":"2021-08-03T06:45:20.853771Z","iopub.status.idle":"2021-08-03T06:45:20.861206Z","shell.execute_reply.started":"2021-08-03T06:45:20.853734Z","shell.execute_reply":"2021-08-03T06:45:20.860417Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"decoder_target = tf.placeholder(dtype=tf.int32, shape=(None,None))\n#decoder_target=tf.compat.v1.placeholder(shape=[None, None], dtype=tf.int32)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:20.862501Z","iopub.execute_input":"2021-08-03T06:45:20.862886Z","iopub.status.idle":"2021-08-03T06:45:20.872560Z","shell.execute_reply.started":"2021-08-03T06:45:20.862851Z","shell.execute_reply":"2021-08-03T06:45:20.871750Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"#model_trainde encoder ve decoderı bağlamıştık zaten\n\nmodel_train.compile(optimizer=optimizer,\n                    loss=sparse_cross_entropy,\n                    target_tensors=[decoder_target])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:20.873787Z","iopub.execute_input":"2021-08-03T06:45:20.874166Z","iopub.status.idle":"2021-08-03T06:45:20.965668Z","shell.execute_reply.started":"2021-08-03T06:45:20.874131Z","shell.execute_reply":"2021-08-03T06:45:20.964913Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"#modelin eğitimi uzun süreceğinden kayıt ediyoruz.\n\n\npath_checkpoint = 'checkpoint.keras'\ncheckpoint = ModelCheckpoint(filepath=path_checkpoint, save_weights_only=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:20.967932Z","iopub.execute_input":"2021-08-03T06:45:20.968252Z","iopub.status.idle":"2021-08-03T06:45:20.973146Z","shell.execute_reply.started":"2021-08-03T06:45:20.968227Z","shell.execute_reply":"2021-08-03T06:45:20.971998Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"try:\n    model_train.load_weights(path_checkpoint)\nexcept Exception as error:\n    print('Checkpoint yüklenirken hata oluştu. Eğitime sıfırdan başlanıyor.')\n    print(error)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:20.974528Z","iopub.execute_input":"2021-08-03T06:45:20.974869Z","iopub.status.idle":"2021-08-03T06:45:20.985272Z","shell.execute_reply.started":"2021-08-03T06:45:20.974836Z","shell.execute_reply":"2021-08-03T06:45:20.984425Z"},"trusted":true},"execution_count":140,"outputs":[{"name":"stdout","text":"Checkpoint yüklenirken hata oluştu. Eğitime sıfırdan başlanıyor.\nUnable to open file (unable to open file: name = 'checkpoint.keras', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# bu inputlar int tokenlerden oluşuyor.\n\nx_data = {'encoder_input': encoder_input_data, 'decoder_input': decoder_input_data}","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:20.986492Z","iopub.execute_input":"2021-08-03T06:45:20.987109Z","iopub.status.idle":"2021-08-03T06:45:20.994148Z","shell.execute_reply.started":"2021-08-03T06:45:20.987059Z","shell.execute_reply":"2021-08-03T06:45:20.993371Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"y_data = {'decoder_output': decoder_output_data}","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:20.995325Z","iopub.execute_input":"2021-08-03T06:45:20.995991Z","iopub.status.idle":"2021-08-03T06:45:21.003060Z","shell.execute_reply.started":"2021-08-03T06:45:20.995955Z","shell.execute_reply":"2021-08-03T06:45:21.002260Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"code","source":"model_train.fit(x=x_data,\n                y=y_data,\n                batch_size=256,\n                epochs=10,\n                callbacks=[checkpoint])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T06:45:21.004897Z","iopub.execute_input":"2021-08-03T06:45:21.005536Z","iopub.status.idle":"2021-08-03T07:29:21.949601Z","shell.execute_reply.started":"2021-08-03T06:45:21.005500Z","shell.execute_reply":"2021-08-03T07:29:21.948708Z"},"trusted":true},"execution_count":143,"outputs":[{"name":"stdout","text":"Train on 473035 samples\nEpoch 1/10\n473035/473035 [==============================] - 268s 568us/sample - loss: 3.1962\nEpoch 2/10\n473035/473035 [==============================] - 264s 558us/sample - loss: 2.2077\nEpoch 3/10\n473035/473035 [==============================] - 264s 558us/sample - loss: 1.9277\nEpoch 4/10\n473035/473035 [==============================] - 264s 557us/sample - loss: 1.8011\nEpoch 5/10\n473035/473035 [==============================] - 263s 556us/sample - loss: 1.7789\nEpoch 6/10\n473035/473035 [==============================] - 264s 557us/sample - loss: 1.7533\nEpoch 7/10\n473035/473035 [==============================] - 263s 556us/sample - loss: 1.7203\nEpoch 8/10\n473035/473035 [==============================] - 263s 556us/sample - loss: 1.6926\nEpoch 9/10\n473035/473035 [==============================] - 263s 557us/sample - loss: 1.6669\nEpoch 10/10\n473035/473035 [==============================] - 263s 556us/sample - loss: 1.6452\n","output_type":"stream"},{"execution_count":143,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f758b69acd0>"},"metadata":{}}]},{"cell_type":"code","source":"# 3 farklı model oluşturduk ama bunların weightleri aynı. model trainin encoderı ile oluşturduğumuz model encoderın weightleri aynı\n#eğitim aşamasında decodera farklı input ve output verdiğimiz için aynı modeli farklı şekillerde tanımlıyoruz.\n\n#model traine ingilizce cümleleri ve bu cümlelerin türkçe çevirileri input olarak vermiştik. modelimizi test ederken verdiğimiz ingilizce \n#cümlenin türkçe çevirisini verirsek sonucun pek anlamıyor olmuyor. model train bizden hem encoder hemde decoder için bizden input istiyor\n\n# ing cümleyi model encodera veriyoruz. bize bir düşünce vektörü verecek elde ettiğimiz düşünce vektörünü ve başlangıç tokenini \n#model decodera veriyoruz.","metadata":{"execution":{"iopub.status.busy":"2021-08-03T07:29:21.950993Z","iopub.execute_input":"2021-08-03T07:29:21.951357Z","iopub.status.idle":"2021-08-03T07:29:21.956776Z","shell.execute_reply.started":"2021-08-03T07:29:21.951319Z","shell.execute_reply":"2021-08-03T07:29:21.955079Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"def translate(input_text, true_output_text=None):\n    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n                                                reverse=True,\n                                                padding='pre')\n    \n    initial_state = model_encoder.predict(input_tokens)\n    \n    max_tokens = tokenizer_dest.max_tokens \n    \n    decoder_input_data = np.zeros(shape=(1, max_tokens), dtype=np.int)  # normalde bachsize 256 ama suan 1 cümle üzerinden çalışıyoruz.\n    \n    token_int = token_start\n    output_text = ''\n    count_tokens = 0\n    \n    while token_int != token_end and count_tokens < max_tokens:\n        decoder_input_data[0, count_tokens] = token_int\n        x_data = {'decoder_initial_state': initial_state, 'decoder_input': decoder_input_data}\n        \n        decoder_output = model_decoder.predict(x_data)\n        \n        token_onehot = decoder_output[0, count_tokens, :]\n        token_int = np.argmax(token_onehot)\n        \n        sampled_word = tokenizer_dest.token_to_word(token_int)\n        output_text += ' ' + sampled_word\n        count_tokens += 1\n        \n    print('Input text:')\n    print(input_text)\n    print()\n    \n    print('Translated text:')\n    print(output_text)\n    print()\n    \n    if true_output_text is not None:\n        print('True output text:')\n        print(true_output_text)\n        print()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T07:29:21.958074Z","iopub.execute_input":"2021-08-03T07:29:21.958438Z","iopub.status.idle":"2021-08-03T07:29:21.968220Z","shell.execute_reply.started":"2021-08-03T07:29:21.958404Z","shell.execute_reply":"2021-08-03T07:29:21.967288Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"translate(input_text=data_src[400000], true_output_text=data_dest[400000])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T07:29:21.969420Z","iopub.execute_input":"2021-08-03T07:29:21.969956Z","iopub.status.idle":"2021-08-03T07:29:22.555174Z","shell.execute_reply.started":"2021-08-03T07:29:21.969920Z","shell.execute_reply":"2021-08-03T07:29:22.554308Z"},"trusted":true},"execution_count":146,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n  warnings.warn('`Model.state_updates` will be removed in a future version. '\n","output_type":"stream"},{"name":"stdout","text":"Input text:\nYou are telling it second hand, aren't you?\n\nTranslated text:\n onu ikinci dakika söylüyorsun değil mi eeee\n\nTrue output text:\nssss Onu dolaylı olarak anlatıyorsun, değil mi? eeee\n\n","output_type":"stream"}]},{"cell_type":"code","source":"translate(input_text='Why people are so rude')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T07:34:18.527767Z","iopub.execute_input":"2021-08-03T07:34:18.528078Z","iopub.status.idle":"2021-08-03T07:34:18.617553Z","shell.execute_reply.started":"2021-08-03T07:34:18.528049Z","shell.execute_reply":"2021-08-03T07:34:18.615622Z"},"trusted":true},"execution_count":158,"outputs":[{"name":"stdout","text":"Input text:\nWhy people are so rude\n\nTranslated text:\n i̇nsanlar neden bu kadar çok kaba eeee\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}